{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31949aed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:52:09.445663Z",
     "iopub.status.busy": "2024-08-12T10:52:09.445257Z",
     "iopub.status.idle": "2024-08-12T10:53:32.338602Z",
     "shell.execute_reply": "2024-08-12T10:53:32.337419Z"
    },
    "papermill": {
     "duration": 82.906373,
     "end_time": "2024-08-12T10:53:32.341220",
     "exception": false,
     "start_time": "2024-08-12T10:52:09.434847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U bitsandbytes\n",
    "%pip install -U transformers\n",
    "%pip install -U accelerate\n",
    "%pip install -U peft\n",
    "%pip install -U trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5009d5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:53:32.360217Z",
     "iopub.status.busy": "2024-08-12T10:53:32.359871Z",
     "iopub.status.idle": "2024-08-12T10:53:33.680877Z",
     "shell.execute_reply": "2024-08-12T10:53:33.680117Z"
    },
    "papermill": {
     "duration": 1.333132,
     "end_time": "2024-08-12T10:53:33.683155",
     "exception": false,
     "start_time": "2024-08-12T10:53:32.350023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "wb_token = user_secrets.get_secret(\"Ming-Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5124328",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:53:33.701615Z",
     "iopub.status.busy": "2024-08-12T10:53:33.701307Z",
     "iopub.status.idle": "2024-08-12T10:53:52.192445Z",
     "shell.execute_reply": "2024-08-12T10:53:52.191600Z"
    },
    "papermill": {
     "duration": 18.503302,
     "end_time": "2024-08-12T10:53:52.195048",
     "exception": false,
     "start_time": "2024-08-12T10:53:33.691746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvivaming\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240812_105335-udcdj75p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mspring-leaf-25\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/vivaming/Fine-tune%20llama3.1%20on%20sentiment%20analysis%20dataset\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/vivaming/Fine-tune%20llama3.1%20on%20sentiment%20analysis%20dataset/runs/udcdj75p\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "wandb.login(key = wb_token)\n",
    "run = wandb.init(\n",
    "project = 'Fine-tune llama3.1 on sentiment analysis dataset',\n",
    "job_type = 'training',\n",
    "anonymous = 'allow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad0e930e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:53:52.215845Z",
     "iopub.status.busy": "2024-08-12T10:53:52.215550Z",
     "iopub.status.idle": "2024-08-12T10:54:11.470973Z",
     "shell.execute_reply": "2024-08-12T10:54:11.470183Z"
    },
    "papermill": {
     "duration": 19.26796,
     "end_time": "2024-08-12T10:54:11.473616",
     "exception": false,
     "start_time": "2024-08-12T10:53:52.205656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 10:54:00.597580: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-12 10:54:00.597692: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-12 10:54:00.708272: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f14be573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:54:11.494778Z",
     "iopub.status.busy": "2024-08-12T10:54:11.493548Z",
     "iopub.status.idle": "2024-08-12T10:54:12.221074Z",
     "shell.execute_reply": "2024-08-12T10:54:12.220256Z"
    },
    "papermill": {
     "duration": 0.740213,
     "end_time": "2024-08-12T10:54:12.223561",
     "exception": false,
     "start_time": "2024-08-12T10:54:11.483348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/sentiment-analysis-for-mental-health-2/Combined Data.csv\",index_col = \"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18dd8616",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:54:12.248283Z",
     "iopub.status.busy": "2024-08-12T10:54:12.247860Z",
     "iopub.status.idle": "2024-08-12T10:54:12.273159Z",
     "shell.execute_reply": "2024-08-12T10:54:12.272256Z"
    },
    "papermill": {
     "duration": 0.038678,
     "end_time": "2024-08-12T10:54:12.275261",
     "exception": false,
     "start_time": "2024-08-12T10:54:12.236583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>3888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bipolar</td>\n",
       "      <td>2877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Depression</td>\n",
       "      <td>15404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Normal</td>\n",
       "      <td>16351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Personality disorder</td>\n",
       "      <td>1201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stress</td>\n",
       "      <td>2669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Suicidal</td>\n",
       "      <td>10653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 status  count\n",
       "0               Anxiety   3888\n",
       "1               Bipolar   2877\n",
       "2            Depression  15404\n",
       "3                Normal  16351\n",
       "4  Personality disorder   1201\n",
       "5                Stress   2669\n",
       "6              Suicidal  10653"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('status').size().reset_index(name = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a62b51d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:54:12.298632Z",
     "iopub.status.busy": "2024-08-12T10:54:12.298197Z",
     "iopub.status.idle": "2024-08-12T10:54:12.335248Z",
     "shell.execute_reply": "2024-08-12T10:54:12.334546Z"
    },
    "papermill": {
     "duration": 0.051144,
     "end_time": "2024-08-12T10:54:12.337803",
     "exception": false,
     "start_time": "2024-08-12T10:54:12.286659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[(df.status != 'Personality disorder') & (df.status !='Stress') & (df.status !='Suicidal')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a86e2a33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:54:12.358213Z",
     "iopub.status.busy": "2024-08-12T10:54:12.357895Z",
     "iopub.status.idle": "2024-08-12T10:54:12.371861Z",
     "shell.execute_reply": "2024-08-12T10:54:12.370573Z"
    },
    "papermill": {
     "duration": 0.026216,
     "end_time": "2024-08-12T10:54:12.373746",
     "exception": false,
     "start_time": "2024-08-12T10:54:12.347530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>3888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bipolar</td>\n",
       "      <td>2877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Depression</td>\n",
       "      <td>15404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Normal</td>\n",
       "      <td>16351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       status  count\n",
       "0     Anxiety   3888\n",
       "1     Bipolar   2877\n",
       "2  Depression  15404\n",
       "3      Normal  16351"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('status').size().reset_index(name = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4cb9855",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:54:12.394273Z",
     "iopub.status.busy": "2024-08-12T10:54:12.393921Z",
     "iopub.status.idle": "2024-08-12T10:54:12.406170Z",
     "shell.execute_reply": "2024-08-12T10:54:12.405233Z"
    },
    "papermill": {
     "duration": 0.024866,
     "end_time": "2024-08-12T10:54:12.408122",
     "exception": false,
     "start_time": "2024-08-12T10:54:12.383256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#shuffle the Dataframe and select only 3000 rows randomly\n",
    "\n",
    "df = df.sample(frac = 1, random_state = 85).reset_index(drop = True).head(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff637986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:54:12.429029Z",
     "iopub.status.busy": "2024-08-12T10:54:12.428389Z",
     "iopub.status.idle": "2024-08-12T10:54:12.433811Z",
     "shell.execute_reply": "2024-08-12T10:54:12.432893Z"
    },
    "papermill": {
     "duration": 0.017898,
     "end_time": "2024-08-12T10:54:12.435794",
     "exception": false,
     "start_time": "2024-08-12T10:54:12.417896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#split the Dataframe\n",
    "train_size = 0.8\n",
    "eval_size = 0.1\n",
    "\n",
    "#calculate the size\n",
    "train_end = int(train_size * len(df))\n",
    "eval_end = train_end + int(eval_size * len(df))\n",
    "\n",
    "#split the data\n",
    "X_train = df[:train_end]\n",
    "X_eval = df[train_end:eval_end]\n",
    "X_test = df[eval_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bb4f778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:54:12.456488Z",
     "iopub.status.busy": "2024-08-12T10:54:12.455773Z",
     "iopub.status.idle": "2024-08-12T10:54:12.461405Z",
     "shell.execute_reply": "2024-08-12T10:54:12.460485Z"
    },
    "papermill": {
     "duration": 0.018163,
     "end_time": "2024-08-12T10:54:12.463439",
     "exception": false,
     "start_time": "2024-08-12T10:54:12.445276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the prompt function\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    return f'''\n",
    "    classify the text into Normal, Depression, Anxiety, Bipolar and return the answer as the corresponding mental health disorder label.\n",
    "    text: {data_point[\"statement\"]}\n",
    "    label:data_point[\"status\"]'''.strip()\n",
    "\n",
    "def generate_test_prompt(data_point):\n",
    "    return f'''\n",
    "    classify the text into Normal, Depression, Anxiety, Bipolar and return the answer as the corresponding mental health disorder label.\n",
    "    text: {data_point[\"statement\"]}\n",
    "    label:'''.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eb20c28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:54:12.484031Z",
     "iopub.status.busy": "2024-08-12T10:54:12.483714Z",
     "iopub.status.idle": "2024-08-12T10:54:12.525010Z",
     "shell.execute_reply": "2024-08-12T10:54:12.524223Z"
    },
    "papermill": {
     "duration": 0.054067,
     "end_time": "2024-08-12T10:54:12.527095",
     "exception": false,
     "start_time": "2024-08-12T10:54:12.473028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate prompt for training and evaluation data\n",
    "X_train = X_train.copy()\n",
    "X_train.loc[:, 'text'] = X_train.apply(generate_prompt, axis=1)\n",
    "\n",
    "X_eval = X_eval.copy()  # Ensure you're working with a copy\n",
    "X_eval.loc[:, 'text'] = X_eval.apply(generate_prompt, axis=1)\n",
    "\n",
    "# Generate prompts for the test data\n",
    "y_true = X_test.loc[:, 'status']\n",
    "X_test = X_test.copy()\n",
    "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ba2f80b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:54:12.547728Z",
     "iopub.status.busy": "2024-08-12T10:54:12.547426Z",
     "iopub.status.idle": "2024-08-12T10:54:12.558367Z",
     "shell.execute_reply": "2024-08-12T10:54:12.557381Z"
    },
    "papermill": {
     "duration": 0.023506,
     "end_time": "2024-08-12T10:54:12.560402",
     "exception": false,
     "start_time": "2024-08-12T10:54:12.536896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "Normal        1028\n",
       "Depression     938\n",
       "Anxiety        258\n",
       "Bipolar        176\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c92102f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:54:12.581246Z",
     "iopub.status.busy": "2024-08-12T10:54:12.580716Z",
     "iopub.status.idle": "2024-08-12T10:54:12.590334Z",
     "shell.execute_reply": "2024-08-12T10:54:12.589387Z"
    },
    "papermill": {
     "duration": 0.022009,
     "end_time": "2024-08-12T10:54:12.592260",
     "exception": false,
     "start_time": "2024-08-12T10:54:12.570251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>status</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>A poem to me, by my bi-polar father. My dad di...</td>\n",
       "      <td>Bipolar</td>\n",
       "      <td>classify the text into Normal, Depression, Anx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2401</th>\n",
       "      <td>Drowning in debt After being on the wrong meds...</td>\n",
       "      <td>Bipolar</td>\n",
       "      <td>classify the text into Normal, Depression, Anx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2402</th>\n",
       "      <td>hi everyone can someone buy me meal i do not h...</td>\n",
       "      <td>Depression</td>\n",
       "      <td>classify the text into Normal, Depression, Anx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2403</th>\n",
       "      <td>Okay, I do not know where to start and it is g...</td>\n",
       "      <td>Depression</td>\n",
       "      <td>classify the text into Normal, Depression, Anx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2404</th>\n",
       "      <td>Bipolar 2 and Sex Addiction (P&amp;amp;M) Hi there...</td>\n",
       "      <td>Bipolar</td>\n",
       "      <td>classify the text into Normal, Depression, Anx...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              statement      status  \\\n",
       "2400  A poem to me, by my bi-polar father. My dad di...     Bipolar   \n",
       "2401  Drowning in debt After being on the wrong meds...     Bipolar   \n",
       "2402  hi everyone can someone buy me meal i do not h...  Depression   \n",
       "2403  Okay, I do not know where to start and it is g...  Depression   \n",
       "2404  Bipolar 2 and Sex Addiction (P&amp;M) Hi there...     Bipolar   \n",
       "\n",
       "                                                   text  \n",
       "2400  classify the text into Normal, Depression, Anx...  \n",
       "2401  classify the text into Normal, Depression, Anx...  \n",
       "2402  classify the text into Normal, Depression, Anx...  \n",
       "2403  classify the text into Normal, Depression, Anx...  \n",
       "2404  classify the text into Normal, Depression, Anx...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e44ccb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:54:12.612951Z",
     "iopub.status.busy": "2024-08-12T10:54:12.612673Z",
     "iopub.status.idle": "2024-08-12T10:54:12.650375Z",
     "shell.execute_reply": "2024-08-12T10:54:12.649552Z"
    },
    "papermill": {
     "duration": 0.050565,
     "end_time": "2024-08-12T10:54:12.652597",
     "exception": false,
     "start_time": "2024-08-12T10:54:12.602032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert to dataset\n",
    "train_data = Dataset.from_pandas(X_train[['text']])\n",
    "eval_data = Dataset.from_pandas(X_eval[['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca8cb296",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:54:12.673940Z",
     "iopub.status.busy": "2024-08-12T10:54:12.673638Z",
     "iopub.status.idle": "2024-08-12T10:54:12.688500Z",
     "shell.execute_reply": "2024-08-12T10:54:12.687522Z"
    },
    "papermill": {
     "duration": 0.027759,
     "end_time": "2024-08-12T10:54:12.690463",
     "exception": false,
     "start_time": "2024-08-12T10:54:12.662704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classify the text into Normal, Depression, Anxiety, Bipolar and return the answer as the corresponding mental health disorder label.\\n    text: The \"Calm\" I have Bipolar 2.\\n\\nAfter a month of isolation and depression, I went out with my friends last night and had a great time. I went home tired and slept the whole day. I woke up exhausted but the racing thoughts and irritability are gone. My mind is blank and surprisingly calm. I\\'m actually panicking a bit because, at this point, I don\\'t know what\\'s happening. There are no more excessive intrusive thoughts, or those crazy ideas - just really calm. I feel alright but I still think of suicide.  I think what scares me a bit is that I experience this before a big mood swing. Am I okay? \\n    label:data_point[\"status\"]',\n",
       " 'classify the text into Normal, Depression, Anxiety, Bipolar and return the answer as the corresponding mental health disorder label.\\n    text: I have had a decent summer, nothing crazy like most kids but it was not bad. I did some stuff and went some places but a while back I went to this public place which had a lot of people.It made me Hella uncomfortable ngl, and I did not realize this, but the anxiety and fear is still there. A significant amount.I thought over summer break I had gotten over my fears and anxieties for the most part but I was wrong, its still there and I am panicking because I am going to have to go back to campus and do stuff around thousands of people.there is going to be a lot of people and I am not ready for it, feel like Ill throw up again.I am scared, terrified actually. please, kill me. tomorrow I have to go to school for registration and stuff like that and I am pretty nervous.\\n    label:data_point[\"status\"]',\n",
       " 'classify the text into Normal, Depression, Anxiety, Bipolar and return the answer as the corresponding mental health disorder label.\\n    text: Stable for years Hey everyone\\n\\nthanks for taking time to read this. \\n\\nI\\'ve question to everyone here who is stable for more than 5 years. \\n\\nWhat are the things that helped you for being stable ?\\n    label:data_point[\"status\"]']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['text'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33a73eb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:54:12.711930Z",
     "iopub.status.busy": "2024-08-12T10:54:12.711588Z",
     "iopub.status.idle": "2024-08-12T10:56:38.934970Z",
     "shell.execute_reply": "2024-08-12T10:56:38.933964Z"
    },
    "papermill": {
     "duration": 146.236938,
     "end_time": "2024-08-12T10:56:38.937381",
     "exception": false,
     "start_time": "2024-08-12T10:54:12.700443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3fa4c17e28845439649b4ab145c3f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_name = \"/kaggle/input/llama-3.1/transformers/8b-instruct/1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"float16\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64af247f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:56:38.959630Z",
     "iopub.status.busy": "2024-08-12T10:56:38.959006Z",
     "iopub.status.idle": "2024-08-12T10:56:39.511450Z",
     "shell.execute_reply": "2024-08-12T10:56:39.510251Z"
    },
    "papermill": {
     "duration": 0.566204,
     "end_time": "2024-08-12T10:56:39.513986",
     "exception": false,
     "start_time": "2024-08-12T10:56:38.947782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b636581e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:56:39.537091Z",
     "iopub.status.busy": "2024-08-12T10:56:39.536427Z",
     "iopub.status.idle": "2024-08-12T10:59:34.943960Z",
     "shell.execute_reply": "2024-08-12T10:59:34.942901Z"
    },
    "papermill": {
     "duration": 175.421147,
     "end_time": "2024-08-12T10:59:34.946113",
     "exception": false,
     "start_time": "2024-08-12T10:56:39.524966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [02:55<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "test=X_test\n",
    "\n",
    "categories = [\"Normal\", \"Depression\", \"Anxiety\", \"Bipolar\"]\n",
    "\n",
    "# Print the first 3 records from X_test\n",
    "for i in tqdm(range(len(test))):\n",
    "    prompt = X_test.iloc[i][\"text\"]\n",
    "    pipe = pipeline(task=\"text-generation\", \n",
    "                        model=model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        max_new_tokens=2, \n",
    "                        temperature=0.1)\n",
    "        \n",
    "    result = pipe(prompt)\n",
    "    input_text = result[0][\"generated_text\"].split(\"label:\")[0]\n",
    "    answer = result[0][\"generated_text\"].split(\"label:\")[-1].strip()\n",
    "    #print(f'{i+1} -- answer: {answer} - input Text: {input_text}')\n",
    "    \n",
    "    #Determine the predicted category\n",
    "    for category in categories:\n",
    "        if category.lower() in answer.lower():\n",
    "            y_pred.append(category)\n",
    "            break\n",
    "    else:\n",
    "        y_pred.append(\"none\")\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd444181",
   "metadata": {
    "papermill": {
     "duration": 0.033205,
     "end_time": "2024-08-12T10:59:35.013854",
     "exception": false,
     "start_time": "2024-08-12T10:59:34.980649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Evaluate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2dc17cd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:59:35.082916Z",
     "iopub.status.busy": "2024-08-12T10:59:35.082557Z",
     "iopub.status.idle": "2024-08-12T10:59:35.089170Z",
     "shell.execute_reply": "2024-08-12T10:59:35.088180Z"
    },
    "papermill": {
     "duration": 0.043226,
     "end_time": "2024-08-12T10:59:35.091055",
     "exception": false,
     "start_time": "2024-08-12T10:59:35.047829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = [\"Normal\", \"Depression\", \"Anxiety\", \"Bipolar\"]\n",
    "mapping = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "def map_func(x):\n",
    "        return mapping.get(x, -1)  # Map to -1 if not found, but should not occur with correct data\n",
    "\n",
    "y_true_mapped = np.vectorize(map_func)(y_true)\n",
    "y_pred_mapped = np.vectorize(map_func)(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6332aab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:59:35.159500Z",
     "iopub.status.busy": "2024-08-12T10:59:35.158784Z",
     "iopub.status.idle": "2024-08-12T10:59:35.166046Z",
     "shell.execute_reply": "2024-08-12T10:59:35.165000Z"
    },
    "papermill": {
     "duration": 0.043586,
     "end_time": "2024-08-12T10:59:35.168006",
     "exception": false,
     "start_time": "2024-08-12T10:59:35.124420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.797\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "print(f'Accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb55081c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:59:35.241332Z",
     "iopub.status.busy": "2024-08-12T10:59:35.240949Z",
     "iopub.status.idle": "2024-08-12T10:59:35.264868Z",
     "shell.execute_reply": "2024-08-12T10:59:35.263796Z"
    },
    "papermill": {
     "duration": 0.061969,
     "end_time": "2024-08-12T10:59:35.266857",
     "exception": false,
     "start_time": "2024-08-12T10:59:35.204888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for label Normal: 0.755\n",
      "Accuracy for label Depression: 0.939\n",
      "Accuracy for label Anxiety: 0.481\n",
      "Accuracy for label Bipolar: 0.667\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.96      0.76      0.84       143\n",
      "  Depression       0.71      0.94      0.81       115\n",
      "     Anxiety       0.59      0.48      0.53        27\n",
      "     Bipolar       0.91      0.67      0.77        15\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       300\n",
      "   macro avg       0.79      0.71      0.74       300\n",
      "weighted avg       0.83      0.80      0.80       300\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[108  29   5   0]\n",
      " [  2 108   4   1]\n",
      " [  3  11  13   0]\n",
      " [  0   4   0  10]]\n"
     ]
    }
   ],
   "source": [
    "# Generate accuracy report\n",
    "unique_labels = set(y_true_mapped)  # Get unique labels\n",
    "    \n",
    "for label in unique_labels:\n",
    "    label_indices = [i for i in range(len(y_true_mapped)) if y_true_mapped[i] == label]\n",
    "    label_y_true = [y_true_mapped[i] for i in label_indices]\n",
    "    label_y_pred = [y_pred_mapped[i] for i in label_indices]\n",
    "    label_accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "    print(f'Accuracy for label {labels[label]}: {label_accuracy:.3f}')\n",
    "        \n",
    "# Generate classification report\n",
    "class_report = classification_report(y_true=y_true_mapped, y_pred=y_pred_mapped, target_names=labels, labels=list(range(len(labels))))\n",
    "print('\\nClassification Report:')\n",
    "print(class_report)\n",
    "    \n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true=y_true_mapped, y_pred=y_pred_mapped, labels=list(range(len(labels))))\n",
    "print('\\nConfusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40378bc1",
   "metadata": {
    "papermill": {
     "duration": 0.099865,
     "end_time": "2024-08-12T10:59:35.401020",
     "exception": false,
     "start_time": "2024-08-12T10:59:35.301155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "364cf287",
   "metadata": {
    "papermill": {
     "duration": 0.033184,
     "end_time": "2024-08-12T10:59:35.467621",
     "exception": false,
     "start_time": "2024-08-12T10:59:35.434437",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Extracting the linear modules names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46714aa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:59:35.537295Z",
     "iopub.status.busy": "2024-08-12T10:59:35.536391Z",
     "iopub.status.idle": "2024-08-12T10:59:35.541661Z",
     "shell.execute_reply": "2024-08-12T10:59:35.540456Z"
    },
    "papermill": {
     "duration": 0.041921,
     "end_time": "2024-08-12T10:59:35.543585",
     "exception": false,
     "start_time": "2024-08-12T10:59:35.501664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c248975",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:59:35.612325Z",
     "iopub.status.busy": "2024-08-12T10:59:35.611960Z",
     "iopub.status.idle": "2024-08-12T10:59:35.769639Z",
     "shell.execute_reply": "2024-08-12T10:59:35.768458Z"
    },
    "papermill": {
     "duration": 0.199665,
     "end_time": "2024-08-12T10:59:35.776880",
     "exception": false,
     "start_time": "2024-08-12T10:59:35.577215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Module: LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model\n",
      "Module: LlamaModel(\n",
      "  (embed_tokens): Embedding(128256, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaSdpaAttention(\n",
      "        (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "        (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "        (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.embed_tokens\n",
      "Module: Embedding(128256, 4096)\n",
      "--------------------------------------------------\n",
      "Name: model.layers\n",
      "Module: ModuleList(\n",
      "  (0-31): 32 x LlamaDecoderLayer(\n",
      "    (self_attn): LlamaSdpaAttention(\n",
      "      (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.0\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.0.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.0.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.0.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.0.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.0.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.0.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.0.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.0.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.0.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.0.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.0.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.0.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.0.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.1\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.1.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.1.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.1.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.1.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.1.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.1.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.1.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.1.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.1.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.1.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.1.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.1.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.1.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.2\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.2.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.2.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.2.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.2.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.2.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.2.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.2.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.2.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.2.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.2.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.2.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.2.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.2.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.3\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.3.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.3.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.3.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.3.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.3.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.3.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.3.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.3.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.3.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.3.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.3.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.3.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.3.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.4\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.4.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.4.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.4.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.4.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.4.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.4.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.4.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.4.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.4.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.4.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.4.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.4.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.4.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.5\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.5.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.5.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.5.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.5.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.5.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.5.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.5.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.5.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.5.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.5.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.5.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.5.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.5.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.6\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.6.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.6.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.6.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.6.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.6.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.6.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.6.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.6.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.6.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.6.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.6.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.6.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.6.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.7\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.7.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.7.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.7.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.7.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.7.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.7.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.7.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.7.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.7.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.7.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.7.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.7.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.7.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.8\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.8.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.8.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.8.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.8.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.8.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.8.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.8.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.8.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.8.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.8.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.8.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.8.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.8.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.9\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.9.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.9.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.9.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.9.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.9.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.9.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.9.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.9.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.9.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.9.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.9.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.9.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.9.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.10\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.10.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.10.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.10.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.10.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.10.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.10.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.10.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.10.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.10.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.10.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.10.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.10.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.10.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.11\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.11.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.11.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.11.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.11.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.11.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.11.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.11.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.11.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.11.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.11.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.11.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.11.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.11.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.12\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.12.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.12.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.12.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.12.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.12.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.12.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.12.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.12.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.12.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.12.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.12.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.12.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.12.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.13\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.13.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.13.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.13.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.13.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.13.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.13.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.13.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.13.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.13.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.13.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.13.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.13.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.13.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.14\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.14.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.14.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.14.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.14.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.14.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.14.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.14.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.14.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.14.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.14.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.14.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.14.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.14.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.15\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.15.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.15.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.15.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.15.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.15.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.15.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.15.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.15.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.15.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.15.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.15.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.15.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.15.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.16\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.16.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.16.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.16.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.16.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.16.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.16.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.16.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.16.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.16.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.16.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.16.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.16.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.16.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.17\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.17.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.17.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.17.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.17.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.17.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.17.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.17.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.17.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.17.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.17.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.17.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.17.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.17.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.18\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.18.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.18.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.18.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.18.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.18.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.18.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.18.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.18.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.18.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.18.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.18.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.18.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.18.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.19\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.19.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.19.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.19.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.19.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.19.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.19.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.19.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.19.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.19.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.19.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.19.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.19.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.19.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.20\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.20.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.20.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.20.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.20.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.20.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.20.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.20.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.20.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.20.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.20.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.20.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.20.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.20.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.21\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.21.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.21.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.21.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.21.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.21.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.21.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.21.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.21.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.21.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.21.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.21.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.21.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.21.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.22\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.22.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.22.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.22.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.22.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.22.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.22.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.22.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.22.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.22.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.22.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.22.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.22.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.22.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.23\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.23.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.23.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.23.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.23.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.23.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.23.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.23.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.23.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.23.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.23.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.23.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.23.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.23.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.24\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.24.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.24.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.24.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.24.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.24.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.24.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.24.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.24.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.24.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.24.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.24.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.24.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.24.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.25\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.25.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.25.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.25.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.25.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.25.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.25.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.25.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.25.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.25.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.25.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.25.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.25.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.25.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.26\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.26.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.26.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.26.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.26.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.26.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.26.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.26.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.26.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.26.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.26.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.26.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.26.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.26.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.27\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.27.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.27.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.27.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.27.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.27.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.27.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.27.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.27.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.27.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.27.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.27.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.27.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.27.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.28\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.28.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.28.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.28.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.28.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.28.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.28.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.28.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.28.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.28.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.28.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.28.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.28.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.28.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.29\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.29.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.29.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.29.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.29.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.29.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.29.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.29.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.29.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.29.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.29.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.29.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.29.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.29.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.30\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.30.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.30.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.30.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.30.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.30.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.30.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.30.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.30.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.30.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.30.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.30.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.30.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.30.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.31\n",
      "Module: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.31.self_attn\n",
      "Module: LlamaSdpaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.31.self_attn.q_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.31.self_attn.k_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.31.self_attn.v_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.31.self_attn.o_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.31.self_attn.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.31.mlp\n",
      "Module: LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "--------------------------------------------------\n",
      "Name: model.layers.31.mlp.gate_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.31.mlp.up_proj\n",
      "Module: Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.31.mlp.down_proj\n",
      "Module: Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.31.mlp.act_fn\n",
      "Module: SiLU()\n",
      "--------------------------------------------------\n",
      "Name: model.layers.31.input_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.layers.31.post_attention_layernorm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.norm\n",
      "Module: LlamaRMSNorm((4096,), eps=1e-05)\n",
      "--------------------------------------------------\n",
      "Name: model.rotary_emb\n",
      "Module: LlamaRotaryEmbedding()\n",
      "--------------------------------------------------\n",
      "Name: lm_head\n",
      "Module: Linear(in_features=4096, out_features=128256, bias=False)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cls = bnb.nn.Linear4bit\n",
    "lora_module_names = set()\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    print(f'Name: {name}')\n",
    "    print(f'Module: {module}')\n",
    "    print('-' * 50)  # A separator for clarity\n",
    "    \n",
    "    if isinstance(module, cls):\n",
    "        names = name.split('.')\n",
    "        lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "        \n",
    "if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "\n",
    "modules =lora_module_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5918c875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T10:59:35.864843Z",
     "iopub.status.busy": "2024-08-12T10:59:35.863856Z",
     "iopub.status.idle": "2024-08-12T10:59:35.870822Z",
     "shell.execute_reply": "2024-08-12T10:59:35.869916Z"
    },
    "papermill": {
     "duration": 0.053349,
     "end_time": "2024-08-12T10:59:35.872850",
     "exception": false,
     "start_time": "2024-08-12T10:59:35.819501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366b944c",
   "metadata": {
    "papermill": {
     "duration": 0.037787,
     "end_time": "2024-08-12T10:59:35.949443",
     "exception": false,
     "start_time": "2024-08-12T10:59:35.911656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca76d68",
   "metadata": {
    "papermill": {
     "duration": 0.038108,
     "end_time": "2024-08-12T10:59:36.028935",
     "exception": false,
     "start_time": "2024-08-12T10:59:35.990827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d6fe815",
   "metadata": {
    "papermill": {
     "duration": 0.039769,
     "end_time": "2024-08-12T10:59:36.110825",
     "exception": false,
     "start_time": "2024-08-12T10:59:36.071056",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "https://www.kaggle.com/code/kingabzpro/fine-tune-llama-3-1-for-text-classification"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5513108,
     "sourceId": 9131060,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 91102,
     "modelInstanceId": 68809,
     "sourceId": 81881,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 452.309124,
   "end_time": "2024-08-12T10:59:38.872721",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-12T10:52:06.563597",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "113f5ea09403464195c790afedd06839": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "23fe3a75049345bbb5e2eed27b149e2d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2ae78f618fc449afaed851f2337261f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4d531d54e3da4bd0b9f1ae3265f551fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4eaf1759f2024d7f8efb154d55e952aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_23fe3a75049345bbb5e2eed27b149e2d",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_ecf9f40ae7254aa6abb3f9c2902e27e0",
       "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
      }
     },
     "879876d0fd59493a896152ed5ee4d43e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2ae78f618fc449afaed851f2337261f0",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c471710600b141d8ae4720614217036c",
       "value": 4.0
      }
     },
     "a6e182f9555f4bc4984acb94d64ff96f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f769eba4cd9f4c7599da782d79066c72",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_4d531d54e3da4bd0b9f1ae3265f551fa",
       "value": "‚Äá4/4‚Äá[02:25&lt;00:00,‚Äá31.58s/it]"
      }
     },
     "c471710600b141d8ae4720614217036c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d3fa4c17e28845439649b4ab145c3f0b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4eaf1759f2024d7f8efb154d55e952aa",
        "IPY_MODEL_879876d0fd59493a896152ed5ee4d43e",
        "IPY_MODEL_a6e182f9555f4bc4984acb94d64ff96f"
       ],
       "layout": "IPY_MODEL_113f5ea09403464195c790afedd06839"
      }
     },
     "ecf9f40ae7254aa6abb3f9c2902e27e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f769eba4cd9f4c7599da782d79066c72": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
